{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A bit of setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils\n",
    "import models\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (16.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Data with Linear Features [hyperparameters will be recorded]\n",
    "In this block, you'll have to implement the complete Linear Regression learning_model. \n",
    "* This includes the `forward()` function which predicts the output for given features, the `backward()` function which trains the weight matrix, and the `loss()` function which calculates the Euclidean Loss.\n",
    "* You'll also have to implement one part of the `calc_features()` function which combines the input data into a single chunk, to be then passed into the `forward()` and `backward()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate artificial data\n",
    "(x_train, y_train, x_val, y_val, x_test, y_test, left_limit, right_limit) = utils.data_generator(utils.linear)\n",
    "\n",
    "# convert all the splits into numpy arrays\n",
    "x_train, y_train = np.array(x_train).reshape(-1,1), np.array(y_train).reshape(-1,1)\n",
    "x_val, y_val = np.array(x_val).reshape(-1,1), np.array(y_val).reshape(-1,1)\n",
    "x_test, y_test = np.array(x_test).reshape(-1,1), np.array(y_test).reshape(-1,1)\n",
    "\n",
    "TRAIN_LOSS = []\n",
    "VAL_LOSS = []\n",
    "no_epochs = 200\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# parameters you can adjust to reduce the validation loss (printed at the end)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "feat_type = 'linear'\n",
    "param = 5 # ignore for linear case\n",
    "lr = 1\n",
    "weight_init = 'random'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# 1. Now you need to implement the 'linear' part of the calc_features() function in the models.py file.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "W_size = models.calc_features(np.array([[0]]), choice=feat_type, param=param).shape[1]\n",
    "\n",
    "# create a model object from the linear_model class\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# 2. Now you need to implement the forward(), backward() and loss() functions in the models.py file.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "learning_model = models.linear_model(W_size=W_size, lr=lr, init=weight_init) \n",
    "\n",
    "# train the model\n",
    "for epoch in range(no_epochs):\n",
    "    feat_train = models.calc_features(x_train, choice=feat_type, param=param)\n",
    "    \n",
    "    # forward pass: calculate the model's prediction on training data\n",
    "    y_pred_train = learning_model.forward(feat_train)\n",
    "    \n",
    "    # compute the loss on training data\n",
    "    loss_train = learning_model.loss(y_train, y_pred_train)\n",
    "    TRAIN_LOSS.append(loss_train)\n",
    "    \n",
    "    # compute loss on validation data\n",
    "    feat_val = models.calc_features(x_val, choice=feat_type, param=param)\n",
    "    y_pred_val = learning_model.forward(feat_val)\n",
    "    loss_val = learning_model.loss(y_val, y_pred_val)\n",
    "    loss_val = learning_model.loss(y_val, y_pred_val)\n",
    "    VAL_LOSS.append(loss_val)\n",
    "    \n",
    "    # backward pass: train the linear regression model, using gradient descent\n",
    "    learning_model.backward(y_train, y_pred_train, feat_train)\n",
    "\n",
    "# plot the training curves, convergence of weight vector, data fit\n",
    "x_pred = np.arange(left_limit, right_limit, 0.01).reshape(-1,1)\n",
    "feat = models.calc_features(x_pred, choice=feat_type, param=param)\n",
    "y_pred = learning_model.forward(feat)\n",
    "\n",
    "utils.plot(TRAIN_LOSS, VAL_LOSS, learning_model.LIST_W.T, x_train, y_train, x_val, y_val, x_test, y_test, x_pred, y_pred)\n",
    "\n",
    "print('Final Validation Loss: {}'.format(loss_val))\n",
    "\n",
    "# save your tuned hyper-parameters\n",
    "utils.save_hyper('linear', feat_type, param, lr, weight_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 1.(b) Tuning the hyper-parameters for 'Linear Data with Linear Features' [hyperparameters will be recorded]\n",
    "Now that your basic linear regression works (i.e. produces the three above graphs: `Learning Curves`, `Convergence of Weight Vector` and `Visualization of the Model Fit`), you should try changing the parameters mentioned above and try to reduce the `Final Validation Loss`. \n",
    "\n",
    "##### Learning Rate\n",
    "* If the weight values shoot up, you are probably using too large a learning rate.\n",
    "* If the weight values change very slowly, the learning rate is too small.\n",
    "\n",
    "##### Weight Initialization\n",
    "* Specifies how the weights are initialized, either randomly or by zeros.\n",
    "\n",
    "##### Param\n",
    "* used to specify parameter values in 'poly' and 'fourier' features. [To be encounterd below]\n",
    "\n",
    "##### Feature Type\n",
    "* which feature type to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Polynomial Data with Linear Features \n",
    "* You just need to experiment with tuning the hyper-parameters in order to minimize the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generate artificial data\n",
    "(x_train, y_train, x_val, y_val, x_test, y_test, left_limit, right_limit) = utils.data_generator(utils.poly)\n",
    "\n",
    "# convert all the splits into numpy arrays\n",
    "x_train, y_train = np.array(x_train).reshape(-1,1), np.array(y_train).reshape(-1,1)\n",
    "x_val, y_val = np.array(x_val).reshape(-1,1), np.array(y_val).reshape(-1,1)\n",
    "x_test, y_test = np.array(x_test).reshape(-1,1), np.array(y_test).reshape(-1,1)\n",
    "\n",
    "TRAIN_LOSS = []\n",
    "VAL_LOSS = []\n",
    "no_epochs = 200\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# 1. Hyper-parameters that you have to adjust in order to minimize the validation loss.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "feat_type = 'linear'\n",
    "param = 6 # ignore for linear case\n",
    "lr = 1\n",
    "weight_init = 'zeros'\n",
    "\n",
    "# create a model object from the linear_model class\n",
    "W_size = models.calc_features(np.array([[0]]), choice=feat_type, param=param).shape[1]\n",
    "learning_model = models.linear_model(W_size=W_size, lr=lr, init=weight_init) \n",
    "\n",
    "for epoch in range(no_epochs):\n",
    "    feat_train = models.calc_features(x_train, choice=feat_type, param=param)\n",
    "    \n",
    "    # forward pass: calculate the model's prediction on training data\n",
    "    y_pred_train = learning_model.forward(feat_train)\n",
    "    \n",
    "    # compute the loss on training data\n",
    "    loss_train = learning_model.loss(y_train, y_pred_train)\n",
    "    TRAIN_LOSS.append(loss_train)\n",
    "    \n",
    "    # compute loss on validation data\n",
    "    feat_val = models.calc_features(x_val, choice=feat_type, param=param)\n",
    "    y_pred_val = learning_model.forward(feat_val)\n",
    "    loss_val = learning_model.loss(y_val, y_pred_val)\n",
    "    loss_val = learning_model.loss(y_val, y_pred_val)\n",
    "    VAL_LOSS.append(loss_val)\n",
    "    \n",
    "    # backward pass: train the linear regression model, using gradient descent\n",
    "    learning_model.backward(y_train, y_pred_train, feat_train)\n",
    "        \n",
    "# plot the training curves, convergence of weight vector, data fit\n",
    "x_pred = np.arange(left_limit, right_limit, 0.01).reshape(-1,1)\n",
    "feat = models.calc_features(x_pred, choice=feat_type, param=param)\n",
    "y_pred = learning_model.forward(feat)\n",
    "\n",
    "utils.plot(TRAIN_LOSS, VAL_LOSS, learning_model.LIST_W.T, x_train, y_train, x_val, y_val, x_test, y_test, x_pred, y_pred)\n",
    "\n",
    "print('Final Validation Loss: {}'.format(loss_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.(b) Reading the learning curves \n",
    "* If Training and Validation curves both decrease similarly, then the model is trained perfectly.\n",
    "* If the Training curve reduces, while the Validation curve increases, the model is overfitted. Best way to tackle this problem is to increase the data size.\n",
    "* In this problem, the model is trained nicely. However, it (the linear model) doesn't have enough representational power to fit this kind of non-linear data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Polynomial Data with Polynomial Features [hyperparameters will be recorded]\n",
    "* In this block, you'll have to implement the `'poly'` part of the `calc_features()` function.\n",
    "* Also tune the hyper-parameters as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate artificial data\n",
    "(x_train, y_train, x_val, y_val, x_test, y_test, left_limit, right_limit) = utils.data_generator(utils.poly)\n",
    "\n",
    "# convert all the splits into numpy arrays\n",
    "x_train, y_train = np.array(x_train).reshape(-1,1), np.array(y_train).reshape(-1,1)\n",
    "x_val, y_val = np.array(x_val).reshape(-1,1), np.array(y_val).reshape(-1,1)\n",
    "x_test, y_test = np.array(x_test).reshape(-1,1), np.array(y_test).reshape(-1,1)\n",
    "\n",
    "TRAIN_LOSS = []\n",
    "VAL_LOSS = []\n",
    "no_epochs = 200\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# 1. Hyper-parameters that you have to adjust in order to minimize the validation loss.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# 2. Now you need to implement the 'poly' part of the calc_features() function in the models.py file.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "feat_type = 'poly'\n",
    "param = 4 # specifies the degree of the fitting polynomial\n",
    "lr = 1\n",
    "weight_init = 'zeros'\n",
    "\n",
    "# create a model object from the linear_model class\n",
    "W_size = models.calc_features(np.array([[0]]), choice=feat_type, param=param).shape[1]\n",
    "learning_model = models.linear_model(W_size=W_size, lr=lr, init=weight_init) \n",
    "\n",
    "for epoch in range(no_epochs):\n",
    "    feat_train = models.calc_features(x_train, choice=feat_type, param=param)\n",
    "    \n",
    "    # forward pass: calculate the model's prediction on training data\n",
    "    y_pred_train = learning_model.forward(feat_train)\n",
    "    \n",
    "    # compute the loss on training data\n",
    "    loss_train = learning_model.loss(y_train, y_pred_train)\n",
    "    TRAIN_LOSS.append(loss_train)\n",
    "    \n",
    "    # compute loss on validation data\n",
    "    feat_val = models.calc_features(x_val, choice=feat_type, param=param)\n",
    "    y_pred_val = learning_model.forward(feat_val)\n",
    "    loss_val = learning_model.loss(y_val, y_pred_val)\n",
    "    loss_val = learning_model.loss(y_val, y_pred_val)\n",
    "    VAL_LOSS.append(loss_val)\n",
    "    \n",
    "    # backward pass: train the linear regression model, using gradient descent\n",
    "    learning_model.backward(y_train, y_pred_train, feat_train)\n",
    "        \n",
    "# plot the training curves, convergence of weight vector, data fit\n",
    "x_pred = np.arange(left_limit, right_limit, 0.01).reshape(-1,1)\n",
    "feat = models.calc_features(x_pred, choice=feat_type, param=param)\n",
    "y_pred = learning_model.forward(feat)\n",
    "\n",
    "utils.plot(TRAIN_LOSS, VAL_LOSS, learning_model.LIST_W.T, x_train, y_train, x_val, y_val, x_test, y_test, x_pred, y_pred)\n",
    "\n",
    "print('Final Validation Loss: {}'.format(loss_val))\n",
    "\n",
    "# save your tuned hyper-parameters\n",
    "utils.save_hyper('poly', feat_type, param, lr, weight_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Periodic Data with Linear Features\n",
    "* You just need to experiment with tuning the hyper-parameters in order to minimize the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate artificial data\n",
    "(x_train, y_train, x_val, y_val, x_test, y_test, left_limit, right_limit) = utils.data_generator(\n",
    "    utils.sawtooth, \n",
    "    train_left=-10, train_right=10, train_size=256,\n",
    "    val_left=11, val_right=15, val_size=64,\n",
    "    test_left=12, test_right=14, test_size=64)\n",
    "\n",
    "# convert all the splits into numpy arrays\n",
    "x_train, y_train = np.array(x_train).reshape(-1,1), np.array(y_train).reshape(-1,1)\n",
    "x_val, y_val = np.array(x_val).reshape(-1,1), np.array(y_val).reshape(-1,1)\n",
    "x_test, y_test = np.array(x_test).reshape(-1,1), np.array(y_test).reshape(-1,1)\n",
    "\n",
    "TRAIN_LOSS = []\n",
    "VAL_LOSS = []\n",
    "no_epochs = 200\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# 1. Hyper-parameters that you have to adjust in order to minimize the validation loss.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "feat_type = 'linear'\n",
    "param = 4 # ignore for linear case\n",
    "lr = 1\n",
    "weight_init = 'zeros'\n",
    "\n",
    "# create a model object from the linear_model class\n",
    "W_size = models.calc_features(np.array([[0]]), choice=feat_type, param=param).shape[1]\n",
    "learning_model = models.linear_model(W_size=W_size, lr=lr, init=weight_init) \n",
    "\n",
    "for epoch in range(no_epochs):\n",
    "    feat_train = models.calc_features(x_train, choice=feat_type, param=param)\n",
    "    \n",
    "    # forward pass: calculate the model's prediction on training data\n",
    "    y_pred_train = learning_model.forward(feat_train)\n",
    "    \n",
    "    # compute the loss on training data\n",
    "    loss_train = learning_model.loss(y_train, y_pred_train)\n",
    "    TRAIN_LOSS.append(loss_train)\n",
    "    \n",
    "    # compute loss on validation data\n",
    "    feat_val = models.calc_features(x_val, choice=feat_type, param=param)\n",
    "    y_pred_val = learning_model.forward(feat_val)\n",
    "    loss_val = learning_model.loss(y_val, y_pred_val)\n",
    "    loss_val = learning_model.loss(y_val, y_pred_val)\n",
    "    VAL_LOSS.append(loss_val)\n",
    "    \n",
    "    # backward pass: train the linear regression model, using gradient descent\n",
    "    learning_model.backward(y_train, y_pred_train, feat_train)\n",
    "        \n",
    "# plot the training curves, convergence of weight vector, data fit\n",
    "x_pred = np.arange(left_limit, right_limit, 0.01).reshape(-1,1)\n",
    "feat = models.calc_features(x_pred, choice=feat_type, param=param)\n",
    "y_pred = learning_model.forward(feat)\n",
    "\n",
    "utils.plot(TRAIN_LOSS, VAL_LOSS, learning_model.LIST_W.T, x_train, y_train, x_val, y_val, x_test, y_test, x_pred, y_pred)\n",
    "\n",
    "print('Final Validation Loss: {}'.format(loss_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Periodic Data with Polynomial Features\n",
    "* You just need to experiment with tuning the hyper-parameters in order to minimize the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate artificial data\n",
    "(x_train, y_train, x_val, y_val, x_test, y_test, left_limit, right_limit) = utils.data_generator(\n",
    "    utils.sawtooth, \n",
    "    train_left=-10, train_right=10, train_size=256,\n",
    "    val_left=11, val_right=15, val_size=64,\n",
    "    test_left=12, test_right=14, test_size=64)\n",
    "\n",
    "# convert all the splits into numpy arrays\n",
    "x_train, y_train = np.array(x_train).reshape(-1,1), np.array(y_train).reshape(-1,1)\n",
    "x_val, y_val = np.array(x_val).reshape(-1,1), np.array(y_val).reshape(-1,1)\n",
    "x_test, y_test = np.array(x_test).reshape(-1,1), np.array(y_test).reshape(-1,1)\n",
    "\n",
    "TRAIN_LOSS = []\n",
    "VAL_LOSS = []\n",
    "no_epochs = 200\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# 1. Hyper-parameters that you have to adjust in order to minimize the validation loss.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "feat_type = 'poly'\n",
    "param = 4 # specifies the degree of the fitting polynomial\n",
    "lr = 1\n",
    "weight_init = 'zeros'\n",
    "\n",
    "# create a model object from the linear_model class\n",
    "W_size = models.calc_features(np.array([[0]]), choice=feat_type, param=param).shape[1]\n",
    "learning_model = models.linear_model(W_size=W_size, lr=lr, init=weight_init) \n",
    "\n",
    "for epoch in range(no_epochs):\n",
    "    feat_train = models.calc_features(x_train, choice=feat_type, param=param)\n",
    "    \n",
    "    # forward pass: calculate the model's prediction on training data\n",
    "    y_pred_train = learning_model.forward(feat_train)\n",
    "    \n",
    "    # compute the loss on training data\n",
    "    loss_train = learning_model.loss(y_train, y_pred_train)\n",
    "    TRAIN_LOSS.append(loss_train)\n",
    "    \n",
    "    # compute loss on validation data\n",
    "    feat_val = models.calc_features(x_val, choice=feat_type, param=param)\n",
    "    y_pred_val = learning_model.forward(feat_val)\n",
    "    loss_val = learning_model.loss(y_val, y_pred_val)\n",
    "    loss_val = learning_model.loss(y_val, y_pred_val)\n",
    "    VAL_LOSS.append(loss_val)\n",
    "    \n",
    "    # backward pass: train the linear regression model, using gradient descent\n",
    "    learning_model.backward(y_train, y_pred_train, feat_train)\n",
    "        \n",
    "# plot the training curves, convergence of weight vector, data fit\n",
    "x_pred = np.arange(left_limit, right_limit, 0.01).reshape(-1,1)\n",
    "feat = models.calc_features(x_pred, choice=feat_type, param=param)\n",
    "y_pred = learning_model.forward(feat)\n",
    "\n",
    "utils.plot(TRAIN_LOSS, VAL_LOSS, learning_model.LIST_W.T, x_train, y_train, x_val, y_val, x_test, y_test, x_pred, y_pred)\n",
    "\n",
    "print('Final Validation Loss: {}'.format(loss_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Periodic Data with Fourier Features [hyperparameters will be recorded]\n",
    "* In this block, you'll have to implement the `'fourier'` part of the `calc_features()` function.\n",
    "* Also tune the hyper-parameters as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate artificial data\n",
    "(x_train, y_train, x_val, y_val, x_test, y_test, left_limit, right_limit) = utils.data_generator(\n",
    "    utils.sawtooth, \n",
    "    train_left=-10, train_right=10, train_size=256,\n",
    "    val_left=11, val_right=15, val_size=64,\n",
    "    test_left=12, test_right=14, test_size=64)\n",
    "\n",
    "# convert all the splits into numpy arrays\n",
    "x_train, y_train = np.array(x_train).reshape(-1,1), np.array(y_train).reshape(-1,1)\n",
    "x_val, y_val = np.array(x_val).reshape(-1,1), np.array(y_val).reshape(-1,1)\n",
    "x_test, y_test = np.array(x_test).reshape(-1,1), np.array(y_test).reshape(-1,1)\n",
    "\n",
    "TRAIN_LOSS = []\n",
    "VAL_LOSS = []\n",
    "no_epochs = 200\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# 1. Hyper-parameters that you have to adjust in order to minimize the validation loss.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# 2. Now you need to implement the 'fourier' part of the calc_features() function in the models.py file.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "feat_type = 'fourier'\n",
    "param = 4 # specifies the number of basic sine and cosine component waves to be used\n",
    "lr = 1\n",
    "weight_init = 'zeros'\n",
    "\n",
    "# create a model object from the linear_model class\n",
    "W_size = models.calc_features(np.array([[0]]), choice=feat_type, param=param).shape[1]\n",
    "learning_model = models.linear_model(W_size=W_size, lr=lr, init=weight_init) \n",
    "\n",
    "for epoch in range(no_epochs):\n",
    "    feat_train = models.calc_features(x_train, choice=feat_type, param=param)\n",
    "    \n",
    "    # forward pass: calculate the model's prediction on training data\n",
    "    y_pred_train = learning_model.forward(feat_train)\n",
    "    \n",
    "    # compute the loss on training data\n",
    "    loss_train = learning_model.loss(y_train, y_pred_train)\n",
    "    TRAIN_LOSS.append(loss_train)\n",
    "    \n",
    "    # compute loss on validation data\n",
    "    feat_val = models.calc_features(x_val, choice=feat_type, param=param)\n",
    "    y_pred_val = learning_model.forward(feat_val)\n",
    "    loss_val = learning_model.loss(y_val, y_pred_val)\n",
    "    loss_val = learning_model.loss(y_val, y_pred_val)\n",
    "    VAL_LOSS.append(loss_val)\n",
    "    \n",
    "    # backward pass: train the linear regression model, using gradient descent\n",
    "    learning_model.backward(y_train, y_pred_train, feat_train)\n",
    "        \n",
    "# plot the training curves, convergence of weight vector, data fit\n",
    "x_pred = np.arange(left_limit, right_limit, 0.01).reshape(-1,1)\n",
    "feat = models.calc_features(x_pred, choice=feat_type, param=param)\n",
    "y_pred = learning_model.forward(feat)\n",
    "\n",
    "utils.plot(TRAIN_LOSS, VAL_LOSS, learning_model.LIST_W.T, x_train, y_train, x_val, y_val, x_test, y_test, x_pred, y_pred)\n",
    "\n",
    "print('Final Validation Loss: {}'.format(loss_val))\n",
    "\n",
    "# save your tuned hyper-parameters\n",
    "utils.save_hyper('periodic', feat_type, param, lr, weight_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Improve above results\n",
    "Now that you have an understanding of the importance of choosing the right features for a given problem, and how to tune the hyperparameters, you can try implementing any kind of features.\n",
    "* You can add additional feature types in the `calc_features()` function.\n",
    "* Select that feature in the `feat_type` in the parameter section.\n",
    "* You will be evaluated on the performance of your models in the sections marked as \"[hyperparameters will be recorded]\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Unknown data [hyperparameters will be recorded]\n",
    "* In this block, you have to come up with a feature type suited for the data given.\n",
    "* One way to proceed can be to visualize the data, and then try already implemented `linear`, `poly` and `fourier` features.\n",
    "* You'll also have to code the whole block yourself.\n",
    "* Finally, specify the parameters that minimize the validation loss. They will then be saved.\n",
    "* Specify four parameters `feat_type`, `param`, `lr`, `weight_init`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training and validation data\n",
    "(x_train, y_train, x_val, y_val) = np.load('linear_reg_data.npy')\n",
    "\n",
    "### YOUR CODE SHOULD BEGIN HERE ###\n",
    "\n",
    "# tune the parameters so as to minimize the validation loss\n",
    "feat_type = None\n",
    "param = None\n",
    "lr = None\n",
    "weight_init = None\n",
    "\n",
    "### YOUR CODE SHOULD END HERE ###\n",
    "\n",
    "# save your tuned hyper-parameters\n",
    "utils.save_hyper('unknown', feat_type, param, lr, weight_init)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
